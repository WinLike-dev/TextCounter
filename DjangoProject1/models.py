from gridfs import GridFS
from pymongo import MongoClient
import os
from collections import Counter

# --- ë°ì´í„°ë² ì´ìŠ¤ ë° ë¶„ì„ ì—”ì§„ ì„¤ì • (ì´ì „ê³¼ ë™ì¼) ---
client = MongoClient('mongodb://localhost:27017/')
db = client['text_counting_project']
fs = GridFS(db)


def analyze_text(text_content):
    if not isinstance(text_content, str): return {}
    words = text_content.split()
    frequency = Counter(words)
    return {
        'total_word_count': len(words),
        'unique_word_count': len(frequency),
        'frequency': dict(frequency.most_common()),
        'status': 'analysis_completed'
    }


class OriginalFile:
    @staticmethod
    def save(filepath):
        filename = os.path.basename(filepath)
        if db.fs.files.find_one({'filename': filename}):
            print(f"ì´ë¯¸ ì›ë³¸ íŒŒì¼ '{filename}'ì´ DBì— ì¡´ì¬í•©ë‹ˆë‹¤.")
            return db.fs.files.find_one({'filename': filename})['_id']
        with open(filepath, 'rb') as f:
            file_id = fs.put(f, filename=filename)
            print(f"ğŸ“œ ì›ë³¸ íŒŒì¼ '{filename}'ì„ DB(GridFS)ì— ë³´ê´€í–ˆìŠµë‹ˆë‹¤.")
            return file_id

    @staticmethod
    def read_content_by_filename(filename):
        try:
            grid_out = fs.find_one({'filename': filename})
            if grid_out:
                return grid_out.read().decode('utf-8')
            return None
        except Exception as e:
            print(f"GridFSì—ì„œ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            return None


class AnalyzedText:
    def __init__(self, analysis_data, source_filename=None):
        self.analysis_data = analysis_data
        self.source_filename = source_filename

    def save(self):
        doc = {'source_filename': self.source_filename, 'analysis_data': self.analysis_data}
        result = db.analyzed_texts.insert_one(doc)
        print(f"âœ… ë¶„ì„ëœ ë°ì´í„°('{self.source_filename}')ê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return result.inserted_id

    @staticmethod
    def find_by_filename(filename):
        return db.analyzed_texts.find_one({'source_filename': filename})

    @staticmethod
    def generate_report_files(document, output_dir='.'):
        """
        DBì—ì„œ ì½ì–´ì˜¨ 'ë¬¸ì„œ'ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if not document:
            print("-> íŒŒì¼ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        analysis_data = document.get('analysis_data', {})
        source_filename = document.get('source_filename', 'ì›ë³¸ íŒŒì¼ ì •ë³´ ì—†ìŒ')

        output_content = f"--- '{source_filename}' ë¶„ì„ ê²°ê³¼ ---\n\n"
        output_content += f"ì „ì²´ ë‹¨ì–´ ìˆ˜: {analysis_data.get('total_word_count', 0)}\n"
        output_content += f"ê³ ìœ  ë‹¨ì–´ ìˆ˜: {analysis_data.get('unique_word_count', 0)}\n"
        output_content += "\n--- ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ ---\n"

        if 'frequency' in analysis_data:
            for word, count in analysis_data['frequency'].items():
                output_content += f"{word} : {count}\n"
        else:
            output_content += "ë¹ˆë„ìˆ˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

        # ì‹¤ì œ íŒŒì¼ë¡œ "ë°°ì¶œ"í•˜ëŠ” ë¶€ë¶„
        output_filepath = os.path.join(output_dir, "output.txt")
        os.makedirs(output_dir, exist_ok=True)
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(output_content)
        print(f"-> ğŸ“„ 'output.txt' íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë°°ì¶œë˜ì—ˆìŠµë‹ˆë‹¤: {output_filepath}")


def process_article(filepath):
    """
    í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ìˆœì„œë„ ë¡œì§ì— ë”°ë¼ ì§€ëŠ¥ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬,
    ìµœì¢… ë¶„ì„ ê²°ê³¼ë¥¼ 'ë°˜í™˜'í•©ë‹ˆë‹¤.
    """
    filename = os.path.basename(filepath)
    print("\n" + "-" * 50)
    print(f"â–¶ï¸ '{filename}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
    existing_analysis = AnalyzedText.find_by_filename(filename)
    if existing_analysis:
        print("-> âœ… ë°œê²¬! ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return existing_analysis
    print("-> âŒ ì—†ìŒ. DBì— ë³´ê´€ëœ ì›ë³¸ íŒŒì¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    archived_content = OriginalFile.read_content_by_filename(filename)
    if archived_content:
        print("-> âœ… ë°œê²¬! DB ì›ë³¸ìœ¼ë¡œ ìƒˆë¡œìš´ ë¶„ì„ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        processed_data = analyze_text(archived_content)
        new_analysis = AnalyzedText(processed_data, filename)
        new_analysis_id = new_analysis.save()
        return AnalyzedText.find_by_filename(filename)  # ì €ì¥ í›„ ë‹¤ì‹œ ì°¾ì•„ ë°˜í™˜
    print("-> âŒ ì—†ìŒ. ë¡œì»¬ íŒŒì¼ì„ ì½ì–´ ì²˜ìŒë¶€í„° ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    if not os.path.exists(filepath):
        print(f"-> âŒ ì˜¤ë¥˜! ë¡œì»¬ ê²½ë¡œì—ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    with open(filepath, 'r', encoding='utf-8') as f:
        local_content = f.read()
    OriginalFile.save(filepath)
    processed_data = analyze_text(local_content)
    new_analysis = AnalyzedText(processed_data, filename)
    new_analysis_id = new_analysis.save()
    return AnalyzedText.find_by_filename(filename)  # ì €ì¥ í›„ ë‹¤ì‹œ ì°¾ì•„ ë°˜í™˜


# --- ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°ì—ë§Œ ì•„ë˜ ì½”ë“œê°€ ë™ì‘ ---
if __name__ == "__main__":

    def run_intelligent_processing_test():
        # --- í…ŒìŠ¤íŠ¸ ì„¤ì • ---
        test_filepath = r"C:\Users\ìŠˆí¼ì»´\Desktop\text file\test.txt"
        report_dir = r"C:\Users\ìŠˆí¼ì»´\Desktop\outt"  # Output íŒŒì¼ì„ ë°°ì¶œí•  í´ë”

        print("\n" + "=" * 50)
        print("ğŸš€ ì§€ëŠ¥í˜• ì²˜ë¦¬ ë° Output íŒŒì¼ ë°°ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        print("=" * 50)

        try:
            db.analyzed_texts.delete_many({})
            db.fs.files.delete_many({})
            db.fs.chunks.delete_many({})
            print("-> í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™” ì™„ë£Œ.")


            # --- ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ì „íˆ ìƒˆë¡œìš´ íŒŒì¼ ì²˜ë¦¬ ---
            print("\n\n--- [ì‹œë‚˜ë¦¬ì˜¤ 1] ìƒˆë¡œìš´ íŒŒì¼ ì²˜ë¦¬ ---")
            result_doc_1 = process_article(test_filepath)
            AnalyzedText.generate_report_files(result_doc_1, report_dir)

            # --- ì‹œë‚˜ë¦¬ì˜¤ 2: ì´ë¯¸ ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” íŒŒì¼ ì²˜ë¦¬ ---
            print("\n\n--- [ì‹œë‚˜ë¦¬ì˜¤ 2] ì´ë¯¸ ë¶„ì„ëœ íŒŒì¼ ì²˜ë¦¬ ---")
            result_doc_2 = process_article(test_filepath)
            AnalyzedText.generate_report_files(result_doc_2, report_dir)

            # --- ì‹œë‚˜ë¦¬ì˜¤ 3: ì›ë³¸ë§Œ ìˆê³  ë¶„ì„ ê²°ê³¼ëŠ” ì—†ëŠ” íŒŒì¼ ì²˜ë¦¬ ---
            print("\n\n--- [ì‹œë‚˜ë¦¬ì˜¤ 3] ì›ë³¸ë§Œ ë³´ê´€ëœ íŒŒì¼ ì²˜ë¦¬ ---")
            db.analyzed_texts.delete_many({})
            print("-> (ìƒí™© ì¡°ì‘: ë¶„ì„ ê²°ê³¼ë§Œ ì‚­ì œë¨)")
            result_doc_3 = process_article(test_filepath)
            AnalyzedText.generate_report_files(result_doc_3, report_dir)

        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # --- í…ŒìŠ¤íŠ¸ ë’·ì •ë¦¬ ---
            #if os.path.exists(test_filepath): os.remove(test_filepath)
            print(f"\n\nâœ¨ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” '{report_dir}' í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


    run_intelligent_processing_test()